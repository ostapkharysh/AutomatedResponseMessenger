{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import random\n",
    "import tarfile\n",
    "import urllib\n",
    "from torchtext import data\n",
    "\n",
    "\n",
    "class TarDataset(data.Dataset):\n",
    "    \"\"\"Defines a Dataset loaded from a downloadable tar archive.\n",
    "\n",
    "    Attributes:\n",
    "        url: URL where the tar archive can be downloaded.\n",
    "        filename: Filename of the downloaded tar archive.\n",
    "        dirname: Name of the top-level directory within the zip archive that\n",
    "            contains the data files.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def download_or_unzip(cls, root):\n",
    "        path = os.path.join(root, cls.dirname)\n",
    "        if not os.path.isdir(path):\n",
    "            tpath = os.path.join(root, cls.filename)\n",
    "            if not os.path.isfile(tpath):\n",
    "                print('downloading')\n",
    "                urllib.request.urlretrieve(cls.url, tpath)\n",
    "            with tarfile.open(tpath, 'r') as tfile:\n",
    "                print('extracting')\n",
    "                tfile.extractall(root)\n",
    "        return os.path.join(path, '')\n",
    "\n",
    "\n",
    "class MR(TarDataset):\n",
    "\n",
    "    url = 'https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz'\n",
    "    filename = 'rt-polaritydata.tar'\n",
    "    dirname = 'rt-polaritydata'\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_key(ex):\n",
    "        return len(ex.text)\n",
    "\n",
    "    def __init__(self, text_field, label_field, path=None, examples=None, **kwargs):\n",
    "        \"\"\"Create an MR dataset instance given a path and fields.\n",
    "\n",
    "        Arguments:\n",
    "            text_field: The field that will be used for text data.\n",
    "            label_field: The field that will be used for label data.\n",
    "            path: Path to the data file.\n",
    "            examples: The examples contain all the data.\n",
    "            Remaining keyword arguments: Passed to the constructor of\n",
    "                data.Dataset.\n",
    "        \"\"\"\n",
    "        def clean_str(string):\n",
    "            \"\"\"\n",
    "            Tokenization/string cleaning for all datasets except for SST.\n",
    "            Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "            \"\"\"\n",
    "            string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "            string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "            string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "            string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "            string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "            string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "            string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "            string = re.sub(r\",\", \" , \", string)\n",
    "            string = re.sub(r\"!\", \" ! \", string)\n",
    "            string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "            string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "            string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "            string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "            return string.strip()\n",
    "\n",
    "        text_field.preprocessing = data.Pipeline(clean_str)\n",
    "        fields = [('text', text_field), ('label', label_field)]\n",
    "\n",
    "        if examples is None:\n",
    "            path = self.dirname if path is None else path\n",
    "            examples = []\n",
    "            with open(os.path.join(path, 'rt-polarity.neg'), errors='ignore') as f:\n",
    "                examples += [\n",
    "                    data.Example.fromlist([line, 'negative'], fields) for line in f]\n",
    "            with open(os.path.join(path, 'rt-polarity.pos'), errors='ignore') as f:\n",
    "                examples += [\n",
    "                    data.Example.fromlist([line, 'positive'], fields) for line in f]\n",
    "        super(MR, self).__init__(examples, fields, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def splits(cls, text_field, label_field, dev_ratio=.1, shuffle=True, root='.', **kwargs):\n",
    "        \"\"\"Create dataset objects for splits of the MR dataset.\n",
    "\n",
    "        Arguments:\n",
    "            text_field: The field that will be used for the sentence.\n",
    "            label_field: The field that will be used for label data.\n",
    "            dev_ratio: The ratio that will be used to get split validation dataset.\n",
    "            shuffle: Whether to shuffle the data before split.\n",
    "            root: The root directory that the dataset's zip archive will be\n",
    "                expanded into; therefore the directory in whose trees\n",
    "                subdirectory the data files will be stored.\n",
    "            train: The filename of the train data. Default: 'train.txt'.\n",
    "            Remaining keyword arguments: Passed to the splits method of\n",
    "                Dataset.\n",
    "        \"\"\"\n",
    "        path = cls.download_or_unzip(root)\n",
    "        examples = cls(text_field, label_field, path=path, **kwargs).examples\n",
    "        if shuffle: random.shuffle(examples)\n",
    "        dev_index = -1 * int(dev_ratio*len(examples))\n",
    "\n",
    "        return (cls(text_field, label_field, examples=examples[:dev_index]),\n",
    "                cls(text_field, label_field, examples=examples[dev_index:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def load_initial_idata(path):\n",
    "    df = []\n",
    "    for line in gzip.open(path, 'rb'):\n",
    "        df.append(eval(line))\n",
    "    return pd.DataFrame.from_dict(df)\n",
    "\n",
    "def clean_str(string):\n",
    "            \"\"\"\n",
    "            Tokenization/string cleaning for all datasets\n",
    "            Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "            \"\"\"\n",
    "            #comment = re.sub(r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", str(comment))\n",
    "            string = string.lower()\n",
    "            string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "            string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "            string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "            string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "            string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "            string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "            string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "            string = re.sub(r\",\", \" , \", string)\n",
    "            string = re.sub(r\"!\", \" ! \", string)\n",
    "            string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "            string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "            #string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "            string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "            return string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A.head()['questionType']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_yesno_questions(dataframe):\n",
    "    with open('customized_data/yesno_questions.txt', 'a') as fl:\n",
    "        for i in range(len(dataframe)):\n",
    "            if dataframe['questionType'][i]=='yes/no':\n",
    "                fl.write(clean_str(dataframe['question'][i])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_openend_questions(dataframe):\n",
    "    with open('customized_data/openend_questions.txt', 'a') as fl:\n",
    "        for i in range(len(dataframe)):\n",
    "            if dataframe['questionType'][i]=='open-ended':\n",
    "                fl.write(clean_str(dataframe['question'][i])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for file in os.listdir(\"amazon\"):\n",
    "    fl = load_initial_data(\"amazon/\"+file)\n",
    "    add_yesno_questions(fl)\n",
    "    add_openend_questions(fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_customized_data(fl):\n",
    "    df = []\n",
    "    with open(fl) as data:\n",
    "        for line in data:\n",
    "            df.append(line.strip())\n",
    "    return pd.DataFrame.from_dict(df)\n",
    "    \n",
    "d = load_customized_data('customized_data/yesno_questions.txt')\n",
    "d.preprocessing = data.Pipeline()\n",
    "#d = list(d[0])\n",
    "#d[0:10]\n",
    "\n",
    "\n",
    "#label_field = \"yes/no\"\n",
    "\n",
    "#fields = [('question', d), ('type_label', label_field)]\n",
    "#data.Dataset(text_field, label_field, path=None, examples=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame\n",
    "a.preprocessing = data.Pipeline()\n",
    "#text_field.preprocessing = data.Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#examples = []\n",
    "#for file in os.listdir(\"customized_data\"):\n",
    "    #examples+=[data.Example.fromlist([line, file[:-14]], fields) for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = \"openend_questions.txt\"\n",
    "c[:-14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictQuestionType(data.Dataset):\n",
    "    def __init__(self, text_field, label_field, **kwargs):\n",
    "        fields = [(\"Question\", text_field), (\"Type\", label_field)]\n",
    "        examples = []\n",
    "        files = os.listdir(\"customized_data\")\n",
    "        labels = [label[:-14] for label in files]\n",
    "        for label in labels:\n",
    "            for fname in files:\n",
    "                with open(\"customized_data/\"+fname, 'r') as f: text = f.readline()\n",
    "                examples.append(data.Example.fromlist([text, label], fields))\n",
    "        super().__init__(examples, fields, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_key(ex): return len(ex.Description)\n",
    "    \n",
    "    @classmethod\n",
    "    def splits(cls, text_field, label_field, root='.',\n",
    "               train='train', test='test', **kwargs):\n",
    "        return super().splits(\n",
    "            root, text_field=text_field, label_field=label_field,\n",
    "            train=train, validation=None, test=test, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'text_field' and 'label_field'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-31293716d449>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcustom_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPredictQuestionType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'text_field' and 'label_field'"
     ]
    }
   ],
   "source": [
    "custom_data = PredictQuestionType(pd.DataFrame.prepocessing, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
