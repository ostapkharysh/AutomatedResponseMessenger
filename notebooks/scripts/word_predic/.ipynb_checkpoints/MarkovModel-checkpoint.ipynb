{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import gzip\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('data/ukrainian_reviews.csv', sep='|')\n",
    "# del data['opinion_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_csv('data/data.txt', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    df = []\n",
    "    for line in gzip.open(path, 'rb'):\n",
    "        df.append(eval(line))\n",
    "    return pd.DataFrame.from_dict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data('data/qa_Appliances.json.gz')\n",
    "df = df['question'] + ' ' + df['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/qa_data.txt', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data_file = 'data/ua_data.txt'\n",
    "training_data_file = 'data/qa_data.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(sentence):\n",
    "    return sentence.translate(str.maketrans('','', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add2dict(dictionary, key, value):\n",
    "    if key not in dictionary:\n",
    "        dictionary[key] = []\n",
    "    dictionary[key].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list2probabilitydict(given_list):\n",
    "    probability_dict = {}\n",
    "    given_list_length = len(given_list)\n",
    "    for item in given_list:\n",
    "        probability_dict[item] = probability_dict.get(item, 0) + 1\n",
    "    for key, value in probability_dict.items():\n",
    "        probability_dict[key] = value / given_list_length\n",
    "    return probability_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_word = {}\n",
    "second_word = {}\n",
    "transitions = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains a Markov model based on the data in training_data_file\n",
    "def train_markov_model():\n",
    "    for line in open(training_data_file):\n",
    "        tokens = remove_punctuation(line.rstrip().lower()).split()\n",
    "        tokens_length = len(tokens)\n",
    "        for i in range(tokens_length):\n",
    "            token = tokens[i]\n",
    "            if i == 0:\n",
    "                initial_word[token] = initial_word.get(token, 0) + 1\n",
    "            else:\n",
    "                prev_token = tokens[i - 1]\n",
    "                if i == tokens_length - 1:\n",
    "                    add2dict(transitions, (prev_token, token), 'END')\n",
    "                if i == 1:\n",
    "                    add2dict(second_word, prev_token, token)\n",
    "                else:\n",
    "                    prev_prev_token = tokens[i - 2]\n",
    "                    add2dict(transitions, (prev_prev_token, prev_token), token)\n",
    "    \n",
    "    # Normalize the distributions\n",
    "    initial_word_total = sum(initial_word.values())\n",
    "    for key, value in initial_word.items():\n",
    "        initial_word[key] = value / initial_word_total\n",
    "        \n",
    "    for prev_word, next_word_list in second_word.items():\n",
    "        second_word[prev_word] = list2probabilitydict(next_word_list)\n",
    "        \n",
    "    for word_pair, next_word_list in transitions.items():\n",
    "        transitions[word_pair] = list2probabilitydict(next_word_list)\n",
    "    \n",
    "    print('Training successful.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Trains a Markov model based on the data in training_data_file\n",
    "# i = 0\n",
    "# for line in open(training_data_file, encoding='utf-8'):\n",
    "#     if i != 5:\n",
    "#         tokens = remove_punctuation(line.rstrip().lower()).split()\n",
    "#         tokens_length = len(tokens)\n",
    "#         for i in range(tokens_length):\n",
    "#             token = tokens[i]\n",
    "#             if len(token) < 13 or (len(token)<2 and (token not in ['a', 'i'])):\n",
    "#                 if i == 0:\n",
    "#                     initial_word[token] = initial_word.get(token, 0) + 1\n",
    "#                 else:\n",
    "#                     prev_token = tokens[i - 1]\n",
    "#                     if i == tokens_length - 1:\n",
    "#                         if len(prev_token) < 2:\n",
    "#                             print(prev_token)\n",
    "#                         add2dict(transitions, (prev_token, token), 'END')\n",
    "#                     if i == 1:\n",
    "#                         add2dict(second_word, prev_token, token)\n",
    "#                     else:\n",
    "#                         prev_prev_token = tokens[i - 2]\n",
    "#                         add2dict(transitions, (prev_prev_token, prev_token), token)\n",
    "#             else:\n",
    "#                 pass\n",
    "#         i += 1\n",
    "#     else:\n",
    "#         break\n",
    "\n",
    "# # Normalize the distributions\n",
    "# initial_word_total = sum(initial_word.values())\n",
    "# for key, value in initial_word.items():\n",
    "#     initial_word[key] = value / initial_word_total\n",
    "\n",
    "# for prev_word, next_word_list in second_word.items():\n",
    "#     second_word[prev_word] = list2probabilitydict(next_word_list)\n",
    "\n",
    "# for word_pair, next_word_list in transitions.items():\n",
    "#     transitions[word_pair] = list2probabilitydict(next_word_list)\n",
    "\n",
    "# print('Training successful.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training successful.\n"
     ]
    }
   ],
   "source": [
    "train_markov_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_word(dictionary):\n",
    "    p0 = np.random.random()\n",
    "    cumulative = 0\n",
    "    for key, value in dictionary.items():\n",
    "        cumulative += value\n",
    "        if p0 < cumulative:\n",
    "            return key\n",
    "    assert(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_sentences = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate sample text\n",
    "def generate():\n",
    "    for i in range(number_of_sentences):\n",
    "        sentence = []\n",
    "        # Initial word\n",
    "        word0 = sample_word(initial_word)\n",
    "        sentence.append(word0)\n",
    "        # Second word\n",
    "        word1 = sample_word(second_word[word0])\n",
    "        sentence.append(word1)\n",
    "        # Subsequent words untill END\n",
    "        while True:\n",
    "            word2 = sample_word(transitions[(word0, word1)])\n",
    "            if word2 == 'END':\n",
    "                break\n",
    "            sentence.append(word2)\n",
    "            word0 = word1\n",
    "            word1 = word2\n",
    "            print(word2)\n",
    "        print(i, ' '.join(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing arena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "problem\n",
      "may\n",
      "be\n",
      "adequate\n",
      "0 ive noticed a problem may be adequate\n",
      "work\n",
      "whirlpool\n",
      "w10130913\n",
      "water\n",
      "pump\n",
      "3363394\n",
      "3348015\n",
      "the\n",
      "one\n",
      "star\n",
      "reviews\n",
      "the\n",
      "basket\n",
      "3\n",
      "14\n",
      "opening\n",
      "thanks\n",
      "this\n",
      "whole\n",
      "unit\n",
      "rodney\n",
      "the\n",
      "ice\n",
      "bin\n",
      "in\n",
      "less\n",
      "than\n",
      "6\n",
      "months\n",
      "1 will this work whirlpool w10130913 water pump 3363394 3348015 the one star reviews the basket 3 14 opening thanks this whole unit rodney the ice bin in less than 6 months\n"
     ]
    }
   ],
   "source": [
    "generate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
